\chapter{Real-Time scheduling and analysis}
In the previous chapter the reader was introduced to some fundamental notions and concepts that will be extensively used throughout the course.

Since, in this chapter, we will focus on the scheduling of real-time tasks and the analysis of each scheduling policy, it is worth mentioning some basic definitions:
\begin{itemize}
\item{\makebox[2cm]{Algorithm\hfill}logical procedure used to solve a problem}
\item{\makebox[2cm]{Program\hfill}formal description of an algorithm, using a programming language}
\item{\makebox[2cm]{Process\hfill}Instance of a program (program in execution)}
\begin{itemize}
\item Program: \side{static entity}
\item Process: \side{dynamic entity}
\end{itemize}
\item the term task is used to indicate a schedulable entity (either a process or a thread), in particular;
\begin{itemize}
\item A thread reresents a flow of execution (it executes with shared resources, multi thread within the same process)
\item A process represents a flow of execution + private resources (it executes with its own resources), such as address space, file table, etc...
\end{itemize}
\end{itemize}

Tasks do not run on bare hardware, but then how can multiple tasks execute on one single CPU?
The \side{OS kernel} is a piece of the operating system that takes care of multi-programming and somehow it is able to create the illusion that each CPU/processor has its own space, whereas in fact it is sharing the same resources with other processes.

In the end the kernel provides the mechanism that enable multiple tasks to execute in parallel; in a sense tasks have the illusion of executing concurrently on a dedicated CPU per task.

On this regard, with the term concurrency we refer to the simultaneous execution of multiple threads/processes in the same PC. 
\side{Concurrency} is implemented by \side{multiplexing} tasks on the same CPU. Tasks are alternated on a real CPU and the \side{task scheduler} decides which task executes at a given instant in time. In other terms, in order to implement the concurrency mechanism it is necessary to introduce this new component (i.e. the task scheduler), since it makes sure that the time of your pc is shared between the different processes or tasks that compete for the resources at that time.

Tasks are associated to temporal constraints (aka deadlines), hence the scheduler must allocate the CPU to tasks so that their deadlines are respected.

\section{Real-Time Scheduling}

One of the key components of an OS and a OS kernel is the scheduler.
A scheduler generates a schedule from a set of tasks. From a mathematical perspective it is a function:

first consider UP systems (given that it has a simpler definition).
A schedule $\sigma(t)$ is a function mapping (every point in) time $t$ into an executing task
\[\sigma : t\,\to\, \mathcal{T} \cup \tau_{idle}\]
where $\mathcal{T}$ os the \side{taskset} and $\tau_{idle}$ is the \side{idle task}.

At the end of the day what the processor/OS kernel does is to implement a function like the scheduler. Clearly, if you have more that one processor, the function under consideration is multivalued, because at every point in time it decide the allocation of the different processors to different tasks.\\
For an SMP system (i.e. $m$ CPUs), $\sigma(t)$ can be extended to map $t$ in vectors $\tau \in (\mathcal{T}\cup \tau_{idle})^m$.

Therefore, formally, the scheduler implements $\sigma(t)$ and as a consequence the scheduler is responsible for selecting the task to execute at time $t$. However, a function is a \side{denotational} way of describing an algorithm, i.e. we specify mathematically what the function is. The \side{operational} way of describing an algorithm on the other hand, is a squence of steps to be taken in order to implement the function.
 
Hence, from an operational/algorithmic point of view:
\begin{itemize}
\item \side{Scheduling algorithm} is an algorithm used to select for each time instant $t$ a task to be executed on a CPU among the ready task
\item Given a task set $\mathcal{T}$, a scheduling algorithm $\mathcal{A}$ generates the schedule $\sigma_{\mathcal{A}}(t)$
\end{itemize} 

In the frame of reference of real-time systems we are interested in finding conditions on the scheduling choice that allow us to meet all the deadlines. Hence, a task set is schedulable by an algorithm $\mathcal{A}$, if by applying that scheduling algorithm, $\sigma_{A}$ does not contain missed deadlines. 

On this note, the \side{Schedulability test} is a way to decide if, given a task set $\mathcal{T}$ and an algorithm $\mathcal{A}$, that task set is going to be schedulable.

The key point here is that given the fact that by making a proper choice of scheduling algorithm, one can obtain a schedulable task set, how can I find a way to schedule the activities so that all the deadlines are met? And this is the problem of real time scheduling.

\section{Cyclic Executive Scheduling}
One way to find a solution of the schedulability problem is to consider a task set of fixed tasks (i.e. you will always have those tasks activated in the very same way).
This is the case of legacy applications and where reliability is fundamental, e.g. military and avionics systems (Air traffic control, Space Shuttle, Boeing 777).

In this scenario, one can obtain a paper and pencil solution, i.e. trying to decide how to schedule things in such a way that the task set is schedulable and stick to that solution.

Of course, there is a more systematic way of solving such problem and such process is called \side{Cyclic Executive Scheduling}.\\
The Cyclic Executive Scheduling (aka \side{timelice scheduling} or \side{cyclic scheduling}) is a scheduling algorithm with very low overhead (in the sense that scheduling decisions are taken-offline) and with a very simple and well tested idea, that was originally used in avionic systems to schedule periodic tasks.

\subsection{The Idea}
Cyclic Executive Scheduling is a
\begin{itemize} 
\item \side{static scheduling algorithm} (i.e. it is decided offline and it is always applied repeatedly in the same way). 

As a consequence, all scheduling decision are taken upfront; it is not something that defers decision to the runtime. 
\item Jobs are assumed not to be preemptable, i.e. a scheduled job executes until termination, it retains control of the CPU for the entire computation time.
\end{itemize}
The procedure to follow in order to apply this scheduling algorithm is:
\begin{enumerate}
\item Split the time axis into slots, each of which is statically allocated to the tasks (\side{scheduling table}).
\item Since we are dealing with periodic tasks, the same sequence of tasks will repeat periodically, hence once the scheduling table is completed, it starts from the beginning and repeats the same sequence over and over again.
\end{enumerate}

A periodic timer activates execution (\side{allocation of a slot}).

In summary three components are needed to uniquely define this scheduling algorithm:
\begin{enumerate}
\item \side{Major Cycle}: least common multiple (lcm) of all the tasks' periods (aka \side{hyperperiod})
\item \side{Minor Cycle}: greatest common divison (gcd) of all the tasks' periods (i.e. granularity at which the scheduling decisions will repeat)
\item \side{A timer}: that fires every Minor Cycle ($\Delta$)
\end{enumerate}

\subsection{Implementation}
As previously mentioned from an execution stand point this scheduling algorithm is extremely simple because every minor cycle:
\begin{enumerate}
\item A periodic timer is initialized
\item Every time the timer fires the scheduler read the scheduling table
\item The scheduler calls the approriate function(s) associated to the corresponding task(s)
\item The scheduler returns to sleep until the next minor cycle (i.e. the timer fires)
\end{enumerate}

\subsection{Advantages}
From it's radical simplicity, allows to create a total check and a certification of the code that can be shipped to a space shuttle or any other avionic application and execute theoretically forever.

An OS is much more difficult to certify, because asynchronious events can be generated and it is impossible to consider upfront every possible course of events that can happen. On the contrary, once you solved the cyclic executing scheduling problem, you have a global solution that is guaranteed to be replicated.

Furthermore, with its simple implementation there is no need for a real-time operating system, no real task exist (just function calls), and only one single stack is necessary for all the \emph{tasks} (since each task runs until completion, there will never be a situation in which the execution of the function are active at the same time).

Since it involves a \side{non-preemptable scheduling} there is no critical races (i.e. there will never be two tasks that shared a variable) and therefore there is no need to protect data (i.e. no need for semaphores, pipes, mutexes, mailboxes, etc...).

Lastly, since there is no need for an OS, all the CPU/execution time is taken by the tasks, hence It has low run-time overhead, and the \side{jitter} (delay in which the result is completed, i.e. difference between when a task start and it finishes ) can be explicitly controlled.
\subsection{Drawbacks}

However, this scheduling algorithm has some important drawbacks:
\begin{itemize}
\item it is not robust w.r.t. overloads. This type of solution assumes that all the tasks do not give back control until they are finished (until its allocated slots terminates). However, if the task execute for much more of its WCET or the allocated time slot, what happens is that the schedule gets disrupted with a subsequent domino effect.
\item It is difficult to expand the schedule (static schedule): introducing a new task requires that the whole system/schedule must be redesigned.
\item Not easy to handle aperiodic/sporadic tasks
\item All task periods must be a multiple of the minor cycle time
\item Difficult to incorporate processes with long periods (big tables)
\item Variable computation time imply that it might be necessary to split tasks into a fixed number of fixed size procedures. This requires to actively change the code into smaller parts, and, in the case of third party library/code, this is not always allowed.
\end{itemize}

\section{Fixed Priority Scheduling}

In general, we want to have more flexibility than the cyclic executive scheduling (in fact it is applied on very niche and particular situations).

What can be done therefore is utilize \side{preemptive scheduling algorithm}s, i.e. algorithm that are allowed to suspend the execution of a task and resume it later.

Therefore a task has, two states: \side{ready} and \side{executing}, and the OS is allowed to enforce a state change.

In the class of preemptive algorithm we will look at different algorithms, but the simplest is the \side{Fixed Priority Scheduling}. In this scheduling algorithm:
\begin{itemize}
\item Every task $\tau_i$, when it is created, is assigned a integer number that is encoding a fixed priority $p_i$
\item The active task with the highest priority is scheduled (POSIX convention)
\end{itemize}
Priorities are integer numbers: the higher the number, the higher the priority (In the research literature, sometimes authors use the opposite convention).

And so, the work of the scheduler at any point in time is to look at the ready queue and pick up the task with the highest priority.

Observations:
\begin{itemize}
\item The response time of the task with the highest priority depends only on its computation time, and therefore it is always less than or equal to its WCET.
\item The response time of tasks with lower priority depends on its own computation time and the \side{interference} of higher priority tasks.
\item Clearly, the scheduling priorities can change how the things behave: are we sure that choosing a different priority assignment policy we would obtain a schedulable task set? Yes.

So, we have two issues: the \side{scheduling analysis} (given a task set and the priorities, is everything schedulable) and \side{syntesis problem} (given the task set and computation time, find the assignment of priorities that guarantees schedulability).
\end{itemize}

\subsection{Priority Assignment}
Given a task set how to assign priorities?\\
Depends on the different objectives that you would like to achieve:
\begin{itemize}
\item \side{Schedulability}, i.e. find the priority assignment that makes all tasks schedulable
\item \side{Response time (optimization)}, i.e. find the priority assignment that minimise the response time of a subset of tasks
\end{itemize}
By now we consider the first objective only, hence we will investigate the \side{optimal priority assignment (Opt)}.

\subsection{Optimal Priority Assignment}

A priority assignment is an optimal priority assignment (Opt):
\begin{itemize}
\item If the task set is schedulable with another priority assignment, then it is schedulable with priority assignment Opt (i.e. the optimal priority assignment cannot do any worse that any other feasible assignment).
\item If the task set is not schedulable with Opt, then it is not schedulable by any other assignment. (If the optimal choice fails then there is no other possible solutions/other alternatives will fail as well).
\end{itemize}

Formally, given a periodic task set $\mathcal{T}$ (all tasks are activated periodically) with all tasks having relative deadline $D_i$ equal to the period $T_i$ ($\forall i, D_i = T_i$), and with all offsets equal to 0 ($\forall i, r_{i,0} = 0$):
\begin{itemize}
\item The best assignment, in this case, is the \side{Rate Monotonic (RM)} assignment
\item Shorter period imply higher priority (hence the name rate monotonic: the priority is monotonic w.r.t. the rate. The higher the frequency/rate, the shorter the period, the higher the priority).
\end{itemize}

Given a periodic task set with deadline different from periods and with all offsets equal to 0 ($\forall i, r_{i,0} = 0$)
\begin{itemize}
\item The best assignment is the \side{Deadline Monotonic (DM)} assignment
\item Shorter relative deadline imply higher priority (the shorter the relative deadline, the higher the priority).
\end{itemize}
Note that the Deadline Monotonic assignment refers to relative deadline (interval in time, fixed value) not absolute deadline (instant in time, change over time).

For sporadic tasks, the same rules are valid as for periodic tasks with offsets equal to  and by considering maximum activation rate.













