\chapter{Timer and Clock Latency}
\definition{Latency}{Measure of the diffence between the theoretical and actual schedule.}
\example{}{A task $\tau$ expects to be scheduled at time $t$, but is actually scheduled at time $t'$.
The resulting latency $L$ takes the form:
\[L = t' - t\]
}

The latency $L$ can be modelled as a blocking time and as such affects the guarantee test.
Similar to what done for shared resources. \\
Blocking time due to latency, not to priority inversion.

Upper bound for $L$? if not known, no schedulability tests!!
The latency must be bounded:
\[\exists L^{max}: L < L^{max}\]
If $L^{max}$ is too high, only few task sets result ot be schedulable.\\
Large blocking time experienced by all tasks!\\
The worst-case latency $L^{max}$ cannot be too high.

A task $\tau_i$ is a stream of jobs $J_{i,j}$ arriving at time $r_{i,j}$.\\
Job $J_{i,j}$ is schedulable at time $t' > r_{i,j}$\\
$t' - r_{i,j}$ is given by:
\begin{enumerate}
    \item $J_{i,j}$'s arrival is signalled at time $r_{i,j} + L^1$
    \item Such event is served at time $r_{i,j} + L^1 + L^2$
    \item $J_{i,j}$ is actually scheduled at $r_{i,j} + L^1 + L^2 + L^3$
\end{enumerate}
where:
\begin{itemize}
    \item $L^1$ is due to the delayed interrupt generation.\\
    Hardware interrupts; generated by devices.\\
    Sometimes, an interrupt should be generated at time $t$ but it is actually generated at time $t' = t + L^{int}$ where $L^{int}$ iis the Interrupt Generation Latency.
    Such latency is due to hardware issues and it is generally small compared to $L^{np}$.
    The only exception is if the device is a timer device, the interrupt generation latency can be quite high
    Timer resolution latency $L^{timer}$
    \item $L^2$ is the non-preemptable section latency ($L^{np}$).\\
    Delay between time when an event is generated and when the kernel handles it.\\
    Due to non-preemptable sections in the kernel, which delay the response to hardware interrupts.\\
    It is composed by various parts: interrupt disabling, bottom halves dealying,\dots\\
    It depends on how the kernel hadles the various events,\dots
    \item $L^3$ is the scheduler latency. Which is the interference from higher priority tasks and its already accounted by the guarantee tests. Hence it will not be considered
\end{itemize}

The Timer Resolution Latency is the interrupt generation latency for a hardware timer device.\\
$L^{timer}$ can often be much larger that the non-preemptable section latency $L^{np}$.\\
Where does it come from? Kernel timers are generally implemented by using a hardware device that produces periodic interrupts.\\
Can we do anything about it?\\
A Periodic timer interrupt is called a tick\\
Example: periodic task (\texttt{setitimer()}, Posix timers, \texttt{clock\_nanosleep()},\dots) $\tau_i$ with period $T_i$\\
Job end $\rightarrow$ $\tau_i$ sleeps for the next activation.\\
Activations are triggered by the periodic interrupt:
\begin{itemize}
    \item Periodic tick interrupt, with period $T^{tick}$
    \item Every $T^{tick}$, the kernel checks if the task must be woken up
    \item If $T_i$ is not multiple of $T^{tick}$, $\tau_i$ experiences a timer resolution latency
\end{itemize}

Traditional operating systems: timer device programmed to generate a periodic interrupt.
\example{}{In a PC, the Programmable Interval Timer (PIT) is programmed in periodic mode}

At every tick the execution enter kernel space.\\
The kernel executes and can:
\begin{itemize}
    \item Wake up tasks
    \item Adjust tasks priorities
    \item Run the scheduler, when returning to user space (possible preemption)
\end{itemize}

Timer interrupt period: trade-off between responsiveness (low latency) and throughput (low overhead).
\begin{itemize}
    \item Large $T^{tick}$: large timer resolution latency
    \item Small $T^{tick}$: high number of interrupts.\\
More switches between US and KS, tasks are interrupted more often and a resulting large overhead
\end{itemize}

For non real-time systems, it is possible to find a reasonable tradeoff but it still depends on the workload.
\example{Linux Kernel}
{
    \begin{itemize}
        \item Linux 2.4: 10 ms (100Hz)
        \item Linux 2.6: 100 Hz, 250 Hz or 1000Hz
        \item Other systems: $T^{tick} = \sfrac{1}{1024}$
    \end{itemize}
}
The timer resoultion latency is experienced by all tasks that want to sleep for a specified time $T$.\\
$\tau_i$ must wake up at time $r_{i,j} = j T_i$, but is woken up at time $t' = \ceil{\cfrac{r_{i,j}}{T^{tick}}}T^{tick}$.

The Timer Resolution Latency is bounded:
\begin{itemize}
    \item $t = r_{i,j}$
    \item $t' = \ceil{\cfrac{r_{i,j}}{T^{tick}}}T^{tick}$
\end{itemize}
\begin{align*}
    L^{timer} &= t' - r_{i,j}\\
&= \ceil{\cfrac{r_{i,j}}{T^{tick}}}T^{tick}- r_{i,j}\\
&= \left(\ceil{\cfrac{r_{i,j}}{T^{tick}}} - \cfrac{r_{i,j}}{T^{tick}}\right) T^{tick} \le T^{tick}
\end{align*}

Reducing $T^{tick}$ below 1ms is generally not acceptable, so periodic tasks can expect a blocking time due to $L^{timer}$ up to 1ms.
How loarge is the effect on the schedulability tests?\\
Additional problems:
\begin{itemize}
    \item Tasks' periods are rounded to multiples of $T^{tick}$
    \item Limit on the minimum task period: $\forall i, T_{i}\ge T^{tick}$
    \item A lot of useless timer interrupts might be generated
\end{itemize}

Remember?
\definition{Timer}{generate an event at a specified time $t$}
\definition{Clock}{keep track of the current system time}

A timer can be used to wake up a periodic task $\tau$, a clock can be used to read the system time (\texttt{gettimeofday()})

\definition{Timer Resolution}{minimum interval at which a periodic timer can fire. If periodic ticks are used, the timer resolution is $T^{tick}$}
\definition{Clock Resolution}{minimum difference between two different timer returned by the clock.}
What's the expected clock resolution?
\begin{itemize}
    \item Traditional OSs use a "tick counter". \\
    Very fast clock: return the number of ticks (jiffies in Linux) from the system boot\\
    Clock resolution : $T^{tick}$
    \item Modern PCs have higher resolution time sources\dots\\
    On x86, TSC (TimeStamp Counter)\\
    High-resolution clock: use the TSC to compute the time since the last timer tick\dots
    \item In summary: high-resolution clocks are easy: every modern OS kernel provide them.
    \item Even using a "traditional" periodic timer tick, it is easy to provide high-resolution clocks: time can be easily read with a high accuracy. 
    \item On the other hand, timer resolution is limited by the system tick $T^{tick}$. It is impossible to generate events at arbitrary instants in time, without latencies
\end{itemize}


\section{Timer Devices}
Timer Devices (e.g. PIT - i8254) generally work in 2 modes: periodic and one-shot.\\
Programmed writing a value $C$ in a counter register.\\
The counter register is decremented at a fixed rate.\\
When the counter is 0, an interrupt is generated:
\begin{itemize}
    \item If the device is programmed in periodic mode, the counter register is automatically reset to the programmed value
    \item If the device is programmed in one-shot mode, the kernel has to explicitly reprogram the device (setting the counter register to a new value)
\end{itemize}

The periodic mode is easier to use! This is why most kernels use it.\\
When using one-shot mode, the timer interrupt handler must:
\begin{enumerate}
    \item Acknowledge the interrupt handlerm, as usual
    \item Check if a timer expired, and do its usual stuff\dots
    \item Compute when the next timer must fire
    \item Reprogram the timer device to generate an interrupt at the correct time
\end{enumerate}
Steps 3 and 4 are particularly critical and difficult.\\
When the kernel reprograms the timer device (step 4), it must know the current time, but the last known time is the time when the interrupt fired (before step 1):
\begin{itemize}
    \item A timer interrupt fires at time $t_1$
    \item The interrupt handler starts (enter KS) at time $t_1'$
    \item Before returning to US, the timer must be reprogrammed, at time $t_1''$
    \item Next interrupt must fire at time $t_2$; the counter register is loaded with $t_2 - t_1$
    \item Next interrupt will fire at $t_2 + (t_1'' - t_1)$
\end{itemize}
The error described previously accumulates with the risk of drift between real time and system time.\\
A free run counter (not stopped at $t_1$) is needed.\\
The counter is synchronised with the timer device and the value of the counter at time $t_1$ is known.\\
This permits to know the time $t_1''$. The new counter register value can be computed correctly.\\
On a PC, the second PIT counter, or the TSC, or the APIC timer can be used as a free run counter.

Serious real-time kernels use high-resolution timers (use hardware time in one-shot mode) which for instance is already implemented in RT-Mach, RTLinux, RTAI and others.\\
General purpose kernels are more concerned about stability and overhead.

Compatibility with "traditional" kernels:
\begin{itemize}
    \item The tick event can be emulated through high-resolution timers
    \item Timer device programmed to generate interrupts both: when needed to serve a timer and at tick boundaries but the "tick" concept is now useless (e.g. Tickless or NO\_HZ system which are good for saving power)  
\end{itemize}