\chapter{Timer and Clock Latency}
The introduction of a Latency is a violation of our expectation of the task: the task expects to be scheduled at a certain time but in effect it is scheduled later.
\definition{Latency}{Measure of the diffence between the theoretical and actual schedule.}
\example{}{A task $\tau$ expects to be scheduled at time $t$, but is actually scheduled at time $t'$.
The resulting latency $L$ takes the form:
\[L = t' - t\]
}

The latency $L$ can be modelled as a blocking time and as such affects the guarantee test and schedulability analysis.
Similar to what done for shared resources. \\
Blocking time due to latency, not to priority inversion.

The problem is: are we able to compute an upper bound for this latency? Otherwise we are not able to apply schedulability tests. Hence, the latency must be bounded:
\[\exists L^{max}: L < L^{max}\]
Otherwise the system is not well-behaved from a real-time perspective.

Formally: consider task $\tau_i$ is a stream of jobs $J_{i,j}$ arriving at time $r_{i,j}$.\\
A job $J_{i,j}$ is schedulable at time $t' > r_{i,j}$. The difference $t' - r_{i,j}$ is composed of three pieces:
\begin{enumerate}
    \item $J_{i,j}$'s arrival is signalled at time $r_{i,j} + L^1$
    \item Such event is served at time $r_{i,j} + L^1 + L^2$
    \item $J_{i,j}$ is actually scheduled at $r_{i,j} + L^1 + L^2 + L^3$
\end{enumerate}
where:
\begin{itemize}
    \item $L^1$ is due to the delayed interrupt generation.\\
    Remember that the hardware interrupts are generated by devices. The device should generate and interrupt at time $t$ but due to some internal issues, this interrupt generation takes place at time $t' = t + L^{int}$ where $L^{int}$ is the Interrupt Generation Latency.\\
    The nature of this latency is due to some hardware issues (there is some time for the hardware to switch the state of some variables and thereby make the processor realize that an interrupt is fired). It is generally small when compared to $L^{np}$, but there is a case in which this interrupt generation is extremely important: the \side{Timer Resolution Latency} $L^{timer}$/
    \item $L^2$ is the \side{Non-preemptable Section Latency} ($L^{np}$).\\
    The non-preemptable section latency is the delay between time when an event is generated and when the kernel handles it. Such delay can be further broken down in several pieces: interrupt disabling, bottom halves dealying,\dots\\
    \item $L^3$ is the \side{Scheduler Latency} or \side{Scheduler Interference}. Which is the interference from higher priority tasks and its already accounted by the guarantee tests. Hence it will not be considered.
\end{itemize}

\section{Timer Resolution Latency}
The latency introduced by the timer $L^{timer}$ can often be much larger that the non-preemptable section latency $L^{np}$ (normally is neglectable).\\
Where does it come from? Timers inside the kernel are activate by a hardware device that produces periodic interrupts. Can we do anything about it?

\subsection{Timer}
A timer device produces periodic interrupts known as \side{Tick}s.
\example{}{Periodic tasks relies on the timer device (\texttt{setitimer()}, Posix timers, \texttt{clock\_nanosleep()},\dots) $\tau_i$ with period $T_i$}
Hence, the tick generation is a basic service where at each tick the kernel checks if some of the task needs to be waken up.
Activations are triggered by the periodic interrupt:
\begin{itemize}
    \item Periodic tick interrupt, with period $T^{tick}$
    \item Every $T^{tick}$, the kernel checks if the task must be woken up
    \item If $T_i$ is not multiple of $T^{tick}$, $\tau_i$ experiences a timer resolution latency
\end{itemize}
If the tick is an integer submultiple of the period there is no issue with the schedulability of the tasks, but if the tick is 5 ms and the period of the task is set at 12ms there is an issue. This is the reason why traditional timers are not suitable for real-time applications. In a traditional operating systems contains a programmable timer device in periodic mode and every tick an interrupt is fired the system enters in kernel mode.
The kernel executes and can:
\begin{itemize}
    \item Wake up tasks
    \item Adjust tasks priorities
    \item Run the scheduler, when returning to user space (possible preemption)
\end{itemize}

There is a clear trade-off between responsiveness (low latency) and throughput (low overhead).
\begin{itemize}
    \item Large $T^{tick}$: large timer resolution latency
    \item Small $T^{tick}$: high number of interrupts, more switches between US and KS, tasks are interrupted more often and therefore the system will have a resulting large overhead
\end{itemize}

For non real-time systems, it is possible to find a reasonable trade-off but it still depends on the workload.
\example{Linux Kernel}
{
    \begin{itemize}
        \item Linux 2.4: 10 ms (100Hz)
        \item Linux 2.6: 100 Hz, 250 Hz or 1000Hz
        \item Other systems: $T^{tick} = \sfrac{1}{1024}$
    \end{itemize}
}

\subsection{Bounded Timer Resolution Latency}
The timer resoultion latency is experienced by all tasks that want to sleep for a specified time $T$.\\
$\tau_i$ must wake up at time $r_{i,j} = j T_i$, but is woken up at time $t' = \ceil{\cfrac{r_{i,j}}{T^{tick}}}T^{tick}$.

The Timer Resolution Latency is bounded:
\begin{itemize}
    \item $t = r_{i,j}$
    \item $t' = \ceil{\cfrac{r_{i,j}}{T^{tick}}}T^{tick}$
\end{itemize}
\begin{align*}
    L^{timer} &= t' - r_{i,j}\\
&= \ceil{\cfrac{r_{i,j}}{T^{tick}}}T^{tick}- r_{i,j}\\
&= \left(\ceil{\cfrac{r_{i,j}}{T^{tick}}} - \cfrac{r_{i,j}}{T^{tick}}\right) T^{tick} \le T^{tick}
\end{align*}
Tipically, tasks are activated every few milliseconds and it is desirable to have a tick period around 2-5 ms. However, reducing $T^{tick}$ below 1ms is generally not acceptable, so periodic tasks can expect a blocking time due to $L^{timer}$ up to 1ms. Hence it is desirable to have periods which are integer multiples of the tick period and this will put limitations on the minimum duration of a period.

Additional problems:
\begin{itemize}
    \item Tasks' periods are rounded to multiples of $T^{tick}$
    \item Limit on the minimum task period: $\forall i, T_{i}\ge T^{tick}$
    \item A lot of useless timer interrupts might be generated
\end{itemize}
Hence, we need a different technology to use instead of the timer device.

\subsection{Clocks}
There is also another important problem: there is some misconception behind Timers and Clocks
\definition{Timer}{A timer generates an event at a specified time $t$}
\definition{Clock}{A clock keeps track of the current system time}
In other words, a clock is needed to keep track of the evolution of time, whereas a timer is used to wake up tasks. They are both importants for real time applications: clocks are needed to take precise measurements on the system, timers are needed to generate activation sequences.

\definition{Timer Resolution}{minimum interval at which a periodic timer can fire. If periodic ticks are used, the timer resolution is $T^{tick}$}
\definition{Clock Resolution}{minimum difference between two different times returned by the clock.}

What's the expected clock resolution?
\begin{itemize}
    \item Traditional OSs use a "tick counter". \\
    OS have a very fast clock and the OS returns the number of ticks (jiffies in Linux) from the system boot. For this reason the clock resolution in these systems is: $T^{tick}$
    \item Modern PCs have higher resolution time sources\dots\\
    On x86, TSC (TimeStamp Counter) inside the processor is a counter that keeps track of the number of clock cycles have been generated since you have turned on the machine.\\
    This clocks are called High-resolution clock: use the TSC to compute the time since the last timer tick\dots and have a precision of around 1 ns.
    \item Even using a "traditional" periodic timer tick, it is easy to provide high-resolution clocks: time can be easily read with a high accuracy. 
    \item On the other hand, timer resolution is limited by the system tick $T^{tick}$. It is impossible to generate events at arbitrary instants in time, without latencies.
\end{itemize}


\section{Timer Devices}
The solution to the timer latency is to use a \side{Timer Device}: instead of using a timer that simply generates interrupts at a specific instant of time, there are programmable timers that can work high resolution (microseconds resolution).
Timer Devices (e.g. PIT - i8254) generally work in 2 modes: periodic and one-shot.

Generally speaking the concept is really simple: this device has a counter and an internal oscillator, every time an oscillation has completed it decrements a counter and only when the counter reaches zero an interrupt is generated. In summary it is the device itself that takes care of decrementing the counter and when to generate the interrupt.

As said before Timer Devices operate in two modes:
\begin{itemize}
    \item If the device is programmed in periodic mode, the counter register is automatically reset to the programmed value
    \item If the device is programmed in one-shot mode, the kernel has to explicitly reprogram the device (setting the counter register to a new value)
\end{itemize}

The periodic mode is easier to use! This is why most kernels use it.\\
When using one-shot mode, the timer interrupt handler must:
\begin{enumerate}
    \item Acknowledge the interrupt handlerm, as usual
    \item Check if a timer expired, and do its usual stuff\dots
    \item Compute when the next timer must fire
    \item Reprogram the timer device to generate an interrupt at the correct time
\end{enumerate}
Steps 3 and 4 are particularly critical and difficult: when the kernel reprograms the timer device (step 4), it must know the current time, but the last known time is the time when the interrupt fired (before step 1).

\begin{itemize}
    \item A timer interrupt fires at time $t_1$
    \item The interrupt handler starts (enter KS) at time $t_1'$
    \item Before returning to US, the timer must be reprogrammed, at time $t_1''$
    \item Next interrupt must fire at time $t_2$; the counter register is loaded with $t_2 - t_1$
    \item Next interrupt will fire at $t_2 + (t_1'' - t_1)$
\end{itemize}
The error described previously accumulates with the risk of drift between real time and system time.\\
A free run counter (not stopped at $t_1$) is needed.\\
The counter is synchronised with the timer device and the value of the counter at time $t_1$ is known.\\
This permits to know the time $t_1''$. The new counter register value can be computed correctly.\\
On a PC, the second PIT counter, or the TSC, or the APIC timer can be used as a free run counter.

Serious real-time kernels use high-resolution timers (use hardware time in one-shot mode) which for instance is already implemented in RT-Mach, RTLinux, RTAI and others.\\
General purpose kernels are more concerned about stability and overhead.

Compatibility with "traditional" kernels:
\begin{itemize}
    \item The tick event can be emulated through high-resolution timers
    \item Timer device programmed to generate interrupts both: when needed to serve a timer and at tick boundaries but the "tick" concept is now useless (e.g. Tickless or NO\_HZ system which are good for saving power)  
\end{itemize}